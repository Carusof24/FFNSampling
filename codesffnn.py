# -*- coding: utf-8 -*-
"""CodesFFNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1szCM93vXutUu_wq2VR1W5AZJ9Oy70Rft

# Feedfoward
"""

import torch
import torch.nn as nn

# Default precision
torch.set_default_dtype(torch.float64)

# -------------------- #
# Feedforward NN Model #
# -------------------- #
class FeedforwardNet(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=100, output_dim=10):
        super(FeedforwardNet, self).__init__()
        # First hidden layer
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        # Secondo hidden layer
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        # Layer di output
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        #these are fully connected layers
        # Activaction function
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)   # Attivazione ReLU sul primo hidden layer
        x = self.fc2(x)
        x = self.relu(x)   # Attivazione ReLU
        x = self.fc3(x)
        x = self.sigmoid(x)  # Sigmoid per l'output
        return x

#wrapper is a class that encapsulates another class
# NNModel, this class provides a wrapper around the FFN class to manage its weights
class NNModel():
    def __init__(self, NN, device='cpu', f=None):
        self.NN = NN
        self.device = device if ('cuda' in device) and torch.cuda.is_available() else 'cpu'
        if f:
            self.load(f)
        else:
            self._to_device()
            self._init_weights()

    def _init_weights(self):
        self.weights = {name: param for name, param in self.NN.named_parameters() if param.requires_grad}

    def copy(self, grad=False):
        if not grad:
            wcopy = {name: self.weights[name].detach().clone() for name in self.weights}
        else:
            wcopy = {name: self.weights[name].grad.detach().clone() for name in self.weights}
        return wcopy

    def set_weights(self, wnew):
        assert all(name in self.weights for name in wnew), f"NNModel.set_weights(): invalid layer found in wnew. Allowed values: {list(self.weights.keys())}"
        for name, new_param in wnew.items():
            for pname, param in self.NN.named_parameters():
                if pname == name:
                    param.data = new_param.detach().clone()
        self._init_weights()

    def load(self, f):
        with open(f, 'rb') as ptf:
            self.NN.load_state_dict(torch.load(ptf, map_location=torch.device(self.device)))
        self._to_device()
        self._init_weights()

    def save(self, f):
        with open(f, 'wb') as ptf:
            torch.save(self.NN.state_dict(), ptf)

    def _to_device(self):
        if 'cuda' in self.device:
            self.NN.to(self.device)

##
# # Feedfoward

# Default precision
torch.set_default_dtype(torch.float64)

# parameters:
input_dim = 100
hidden_dim = 100
output_dim = 10

# Initialize the neural network
net = FeedforwardNet(input_dim, hidden_dim, output_dim)
model = NNModel(net)


# Get the initial weights
initial_weights = model.copy()


# Modify the weights
modified_weights = model.copy()

# change the weight of the first layer
for name, param in modified_weights.items():
    if name == 'fc1.weight':
        param.data = torch.randn_like(param.data)
        break


# Set the modified weights back into the model
model.set_weights(modified_weights)


# Verify that the weights in the NN and the weights stored in model.weights are the same
for name, param in model.NN.named_parameters():
    if name in modified_weights:
      print(f"Layer: {name}")
      print("Difference between model.weights and NN parameters:", torch.equal(model.weights[name], param.data))

# Reset weights to initial values (example)
model.set_weights(initial_weights)

# Verify that the weights have been reset correctly
for name, param in model.NN.named_parameters():
    if name in initial_weights:
      print(f"Layer: {name}")
      print("Difference between model.weights and NN parameters after reset:", torch.equal(model.weights[name], param.data))

#Manage e manipulate the weights of neural network using NNModel.
#Make changes to the weights (simulating a "move" in a search or optimization process).
#Revert changes to the weights if needed (like rejecting a move).
# Inizializza la rete e il wrapper
net = FeedforwardNet()
nn_model = NNModel(net)

# Salvataggio dei pesi iniziali
w_initial = nn_model.copy()

# Stampa dei pesi iniziali del layer fc1
print("Pesi iniziali di fc1.weight:")
print(nn_model.weights['fc1.weight'])

# Simulazione di una mossa, modifica del peso fc1.weight
wnew = nn_model.copy()
wnew['fc1.weight'] = wnew['fc1.weight'] + 1.0

# Applica la mossa proposta
nn_model.set_weights(wnew)
print("\nPesi dopo la mossa proposta di fc1.weight:")
print(nn_model.weights['fc1.weight'])

# Supponiamo di voler rifiutare la mossa: ripristino della configurazione precedente
nn_model.set_weights(w_initial)
print("\nPesi dopo il ripristino (rifiuto della mossa) di fc1.weight:")
print(nn_model.weights['fc1.weight'])

"""# Dataset"""

import numpy as np
import torch
from torch.utils.data import Dataset

def flip_spin(vector, p=0.1):
    """
    Data una sequenza di spin (±1), inverte ciascun elemento con probabilità p.
    """
    flip_mask = np.random.rand(vector.shape[0]) < p  # Quali spin invertire
    vector_flipped = vector.copy()
    vector_flipped[flip_mask] *= -1  # Inversione degli spin selezionati
    return vector_flipped

def generate_dataset(n_vectors=10, vector_dim=100, p_flip=0.1):
    """
    Genera un dataset di n_vectors vettori, ognuno di 10dimensionale
    Ogni vettore contiene spin ±1, e viene flippato con probabilità p_flip.
    """
    dataset = []
    labels = []

    for k in range(n_vectors):
        s_k = np.random.choice([-1, 1], size=vector_dim)  # Vettore iniziale
        s_i = flip_spin(s_k, p_flip)  # Flippo
        dataset.append(s_i)

        # Creazione della label one-hot (10 classi)
        label = np.zeros(n_vectors)
        label[k] = 1
        labels.append(label)

    return np.array(dataset), np.array(labels)

class SpinDataset(Dataset):
    """
    Dataset compatibile con PyTorch, restituisce coppie (input, target).
    """
    def __init__(self, n_vectors=10, vector_dim=100, p_flip=0.1):
        X, y = generate_dataset(n_vectors, vector_dim, p_flip)
        self.X = torch.tensor(X, dtype=torch.float64)  # Input
        self.y = torch.tensor(y, dtype=torch.float64)  # One-hot label

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Test dataset
if __name__ == "__main__":
    dataset = SpinDataset(n_vectors=10, vector_dim=100, p_flip=0.1)
    print("Esempio di input:", dataset[0][0])  # Un vettore di spin
    print("Esempio di label:", dataset[3][1])  # La label one-hot

"""# training with ADAM"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader


# Imposta il dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Inizializza il modello e spostalo sul dispositivo
model = FeedforwardNet(input_dim=100, hidden_dim=100, output_dim=10).to(device)

# Crea il dataset e il dataloader
dataset = SpinDataset(n_vectors=10, vector_dim=100, p_flip=0.1)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Definisci la funzione di perdita
# Usiamo la Binary Cross Entropy, compatibile con l'output Sigmoid e le label one-hot
criterion = nn.BCELoss()

# Inizializza l'ottimizzatore Adam
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Numero di epoche di training
num_epochs = 100

print("Inizio training con Adam...")
for epoch in range(num_epochs):
    running_loss = 0.0
    for inputs, labels in dataloader:
        # Sposta dati sul dispositivo (CPU o GPU)
        inputs, labels = inputs.to(device), labels.to(device)

        # Azzeriamo i gradienti
        optimizer.zero_grad()

        # Forward pass: calcola l'output della rete
        outputs = model(inputs)

        # Calcola la perdita
        loss = criterion(outputs, labels)

        # Backpropagation
        loss.backward()

        # Aggiornamento dei pesi
        optimizer.step()

        running_loss += loss.item()

    # Stampa la perdita media per epoca
    print(f"Epoca {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}")

print("Training completato!")

"""# training with SGD



"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import argparse
import math
import time

# Funzione di costo e metrica
cost_fn = nn.MSELoss()
def metric_fn(outputs, targets):
    # Calcola l'accuracy confrontando l'indice della massima attivazione
    pred = torch.argmax(outputs, dim=1)
    true = torch.argmax(targets, dim=1)
    return (pred == true).float().mean()

# ------------------------------- #
# Trainer SGD per il Training    #
# ------------------------------- #
class SGDTrainer:
    def __init__(self, nn_model, dataset, cost_fn, metric_fn, lr=1.0, lamda=1.0e-7, device='cpu',
                 results_dir="results", prefix="w", save_step=1, print_step=100, wsave_step=100, wsave_dmet=0.005):
        self.nn_model = nn_model
        self.dataset = dataset
        self.cost_fn = cost_fn
        self.metric_fn = metric_fn
        self.lr = lr
        self.lamda = lamda
        self.device = device if ('cuda' in device and torch.cuda.is_available()) else 'cpu'
        self.results_dir = results_dir
        self.prefix = prefix
        self.save_step = save_step
        self.print_step = print_step
        self.wsave_step = wsave_step
        self.wsave_dmet = wsave_dmet

        self.nn_model.NN.to(self.device)
        self.optimizer = optim.SGD(self.nn_model.NN.parameters(), lr=self.lr)
        self.data = {'epoch': 0, 'loss': 0.0, 'metric': 0.0, 'time': 0.0}

        # Crea le directory per i risultati
        os.makedirs(results_dir, exist_ok=True)
        self.weights_dir = os.path.join(results_dir, "weights")
        os.makedirs(self.weights_dir, exist_ok=True)

    def _save(self, header=False):
        data_file = os.path.join(self.results_dir, 'data.dat')
        if header:
            with open(data_file, 'w') as f:
                header_line = '\t'.join(self.data.keys())
                line = '\t'.join(str(v) for v in self.data.values())
                f.write(header_line + '\n')
                f.write(line + '\n')
        else:
            with open(data_file, 'a') as f:
                line = '\t'.join(str(v) for v in self.data.values())
                f.write(line + '\n')

    def _wsave(self, epoch):
        weight_file = os.path.join(self.weights_dir, f"{self.prefix}{epoch}.pt")
        torch.save(self.nn_model.NN.state_dict(), weight_file)

    def _status(self):
        line = f"Epoch {self.data['epoch']}\tLoss: {self.data['loss']:.5f}\tMetric: {self.data['metric']:.5f}"
        print(line)

    def train(self, epochs=1000, batch_size=1):
        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        # Salva header iniziale
        self._save(header=True)
        self._wsave(0)
        start_time = torch.cuda.Event(enable_timing=True) if self.device != 'cpu' else None
        if start_time is not None:
            start_time.record()

        for epoch in range(1, int(epochs) + 1):
            epoch_loss = 0.0
            epoch_metric = 0.0
            for inputs, targets in dataloader:
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                self.optimizer.zero_grad()
                outputs = self.nn_model.NN(inputs)
                loss = self.cost_fn(outputs, targets)
                # Termine di regolarizzazione
                mod2 = sum(torch.sum(param**2) for param in self.nn_model.NN.parameters())
                loss = loss + (self.lamda/2.0)*mod2
                loss.backward()
                self.optimizer.step()
                epoch_loss += loss.item()
                with torch.no_grad():
                    metric_val = self.metric_fn(outputs, targets)
                    batch_metric = metric_val.item() if isinstance(metric_val, torch.Tensor) else metric_val
                    epoch_metric += batch_metric

            # Aggiornamento dati training
            self.data['epoch'] = epoch
            self.data['loss'] = epoch_loss / len(dataloader)
            self.data['metric'] = epoch_metric / len(dataloader)
            self.data['time'] = 0.0  # Puoi implementare il calcolo del tempo se desideri

            if epoch % self.print_step == 0 or epoch == 1:
                self._status()
            if epoch % self.save_step == 0:
                self._save()
            if epoch % self.wsave_step == 0:
                self._wsave(epoch)
        print("\nTraining completato!")

    def _status(self):
        line = f"Epoch {self.data['epoch']}\tLoss: {self.data['loss']:.5f}\tMetric: {self.data['metric']:.5f}"
        print(line)

# ------------------------------- #
# Main: Setup e avvio del training
# ------------------------------- #
def main(args):
    print(f"Starting training on device: {args.device}")
    print(f"Epochs: {args.epochs}, Learning Rate: {args.lr}, Regularization lamda: {args.lamda}\n")

    # Inizializzazione del modello e del wrapper
    net = FeedforwardNet(input_dim=100, hidden_dim=100, output_dim=10)
    nn_model = NNModel(NN=net, device=args.device)

    # Inizializzazione del dataset
    dataset = SpinDataset(n_vectors=10, vector_dim=100, p_flip=0.1)

    # Inizializzazione del trainer
    trainer = SGDTrainer(
        nn_model=nn_model,
        dataset=dataset,
        cost_fn=cost_fn,
        metric_fn=metric_fn,
        lr=args.lr,
        lamda=args.lamda,
        device=args.device,
        results_dir=args.results_dir,
        prefix=args.prefix,
        save_step=args.save_step,
        print_step=args.print_step,
        wsave_step=args.wsave_step,
        wsave_dmet=args.wsave_dmet,
    )

    # Avvia il training
    trainer.train(epochs=args.epochs, batch_size=1)

def create_parser():
    parser = argparse.ArgumentParser(description="Training della Feedforward NN con SGD")
    # Parametri principali
    parser.add_argument("--epochs", type=float, default=1.0e+3, help="Numero di epoche di training")
    parser.add_argument("--lr", type=float, default=1.0, help="Learning rate")
    parser.add_argument("--lamda", type=float, default=1.0e-7, help="Coefficiente del termine di regolarizzazione")
    # Extra inputs
    parser.add_argument("--results_dir", type=str, default="results", help="Directory dei risultati")
    parser.add_argument("--prefix", type=str, default="w", help="Prefisso per il salvataggio dei pesi (<prefix><epoch>.pt)")
    parser.add_argument("--save_step", type=int, default=1, help="Numero di epoche dopo le quali salvare i dati di training")
    parser.add_argument("--print_step", type=int, default=100, help="Numero di epoche dopo le quali stampare lo stato del training")
    parser.add_argument("--wsave_step", type=int, default=100, help="Numero di epoche dopo le quali salvare il file dei pesi")
    parser.add_argument("--wsave_dmet", type=float, default=0.005, help="Variazione minima della metrica per salvare i pesi")
    parser.add_argument("--device", type=str, default="cpu", help="Device per il training (cpu o cuda)")
    return parser

if __name__ == "__main__":
    parser = create_parser()
    args, unknown = parser.parse_known_args()
    main(args)