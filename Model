import torch
import torch.nn as nn

# Default precision
torch.set_default_dtype(torch.float64)

# -------------------- #
# Feedforward NN Model #
# -------------------- #
class FeedforwardNet(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=100, output_dim=10):
        super(FeedforwardNet, self).__init__()
        # First hidden layer
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        # Secondo hidden layer
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        # Layer di output
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        #these are fully connected layers
        # Activaction function
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)   # Attivazione ReLU sul primo hidden layer
        x = self.fc2(x)
        x = self.relu(x)   # Attivazione ReLU 
        x = self.fc3(x)
        x = self.sigmoid(x)  # Sigmoid per l'output
        return x

#wrapper is a class that encapsulates another class 
# NNModel, this class provides a wrapper around the FFN class to manage its weights 
class NNModel():
    def __init__(self, NN, device='cpu', f=None):
        self.NN = NN
        self.device = device if ('cuda' in device) and torch.cuda.is_available() else 'cpu'
        if f:
            self.load(f)
        else:
            self._to_device()
            self._init_weights()

    def _init_weights(self):
        self.weights = {name: param for name, param in self.NN.named_parameters() if param.requires_grad}

    def copy(self, grad=False):
        if not grad:
            wcopy = {name: self.weights[name].detach().clone() for name in self.weights}
        else:
            wcopy = {name: self.weights[name].grad.detach().clone() for name in self.weights}
        return wcopy

    def set_weights(self, wnew):
        assert all(name in self.weights for name in wnew), f"NNModel.set_weights(): invalid layer found in wnew. Allowed values: {list(self.weights.keys())}"
        for name, new_param in wnew.items():
            for pname, param in self.NN.named_parameters():
                if pname == name:
                    param.data = new_param.detach().clone()
        self._init_weights()

    def load(self, f):
        with open(f, 'rb') as ptf:
            self.NN.load_state_dict(torch.load(ptf, map_location=torch.device(self.device)))
        self._to_device()
        self._init_weights()

    def save(self, f):
        with open(f, 'wb') as ptf:
            torch.save(self.NN.state_dict(), ptf)

    def _to_device(self):
        if 'cuda' in self.device:
            self.NN.to(self.device)
