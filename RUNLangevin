# Updated script: compute mean loss & stderr on 2nd block and tag each run uniquely
import os
from datetime import datetime
import torch.nn.functional as F
import numpy as np

device       = 'cpu'
# Simulation blocks
stime_list    = [4.0e4, 2.0e4]        # simulated times (moves = stime/dt)
T_list        = [1.0e-6, 1.0e-6]      # target temperatures
Tratio_list   = [3.0e-1, 1e-1]      # ratio T_mb / T
dt_list       = [1,    1]             # integration time-step
m1_list       = [0.2,  0.8]           # mobility (c1 = sqrt(1 - m1^2))

# Unique run identifier
dt = datetime.now().strftime("%Y%m%d_%H%M%S")
run_tag = f"run_{dt}"

# Paths
base_results = '/content/drive/MyDrive/pl_results'
base_weights = '/content/drive/MyDrive/pl_weights'
results_dir  = os.path.join(base_results, run_tag)
weights_dir  = os.path.join(base_weights, run_tag)
os.makedirs(results_dir, exist_ok=True)
os.makedirs(weights_dir, exist_ok=True)

# Initialize model, dataset, sampler, etc. 
# ...

net   = FeedforwardNet(input_dim=100, hidden_dim=100, output_dim=10)
model = NNModel(net)

# Static sampling parameters
pars = {
    'lamda':        1.0e-5,   # regularization coefficient
    'mbs':          64, # mini-batch size
    'max_extractions': 5000,  # max extractions for std estimate
    'extractions':     200,   # extractions per std cycle
    'threshold_est':  0.01,   # std convergence threshold
    'max_adj_step': 50000,    # max steps before re-adjourn
    'min_adj_step':  1000,    # base step for re-adjourn
    'opt_streak':      5,     # streak scale for adjustment
    'threshold_adj':  0.01,   # threshold for streak
    'seed':            0,     # RNG seed
    'lob':             False,
}
import torch.nn.functional as F
alpha= 1.0
# Calcola il numero di parametri del modello
N = sum(p.numel() for p in model.NN.parameters() if p.requires_grad)
P= int(alpha*N) # Convert P to an integer using int()

dataset = make_fashion_subset(P, seed=42)

# Initialize random generator
generator = init_torch_generator(seed=int(pars['seed']), device=device)
# Load pretrained weights OF ADAM TRAINER
#  1. monta Google Drive

final_weights_path = "/content/drive/MyDrive/weights/final_model_1.pt"
# Ensure the file exists before attempting to load
if os.path.exists(final_weights_path):
    model.load(final_weights_path)
else:
    print(f"Warning: Pretrained weights file not found at {final_weights_path}. Starting with randomly initialized weights.")
 # Define cost and metric
Cost   = lambda logits, target: F.cross_entropy(logits, target)
Metric = lambda logits, target: (logits.argmax(dim=1) == target).float().mean()
# Instantiate sampler
sampler = PLSampler(
    model=model,
    Cost=Cost,
    Metric=Metric,
    dataset=dataset,
    Y_full=Y_full,
    generator=generator,
    name=f'PL_sampler',
)


# Loop over each simulation block
for idx in range(len(stime_list)):
    # Dynamic block parameters
    stime  = stime_list[idx]
    T      = T_list[idx]
    Tratio = Tratio_list[idx]
    dt     = dt_list[idx]
    m1     = m1_list[idx]

    # Compute moves and temperatures
    moves       = int(stime / dt)
    pars['moves'] = moves
    pars['dt']    = dt
    pars['T']     = T
    pars['T_mb']  = Tratio * T
    pars['m1']    = m1


    # Define cost and metric
    Cost   = lambda logits, target: F.cross_entropy(logits, target)
    Metric = lambda logits, target: (logits.argmax(dim=1) == target).float().mean()


# Storage for block losses
block_losses = {idx: [] for idx in range(len(stime_list))}

for idx in range(len(stime_list)):
    stime, T, Tratio, dt, m1 = (
        stime_list[idx], T_list[idx], Tratio_list[idx], dt_list[idx], m1_list[idx]
    )
    moves = int(stime / dt)
    pars.update({'moves': moves, 'dt': dt, 'T': T, 'T_mb': Tratio * T, 'm1': m1})

    # Run sampling and collect history
    history = sampler.sample(
        pars=pars,
        results_dir=results_dir,
        weights_dir=weights_dir,
        prefixes={'w': f"w_block{idx}", 'm': f"m_block{idx}"},
        start=None,
        keep_going=False,
        save_step=100,
        check_step=100,
        wsave_step=max(1, moves // 100),
        print_step=1000
    )
     # Extract the 'loss' value from each dictionary in the history list
    losses = np.array([step_data['loss'] for step_data in history])
    block_losses[idx] = losses

if 1 in block_losses and len(block_losses[1]) > 0:
    losses2 = block_losses[1]
    mean_loss2 = losses2.mean()
    stderr_loss2 = losses2.std(ddof=1) / np.sqrt(len(losses2))

    print(f"\nSecond block (idx=1) mean loss: {mean_loss2:.6f} Â± {stderr_loss2:.6f}")

# Save stats to a text file
with open(os.path.join(results_dir, 'block2_loss_stats.txt'), 'w') as f:
    f.write(f"mean_loss: {mean_loss2:.6f}\n")
    f.write(f"stderr_loss: {stderr_loss2:.6f}\n")

print(f"Results and stats saved under {results_dir}")
